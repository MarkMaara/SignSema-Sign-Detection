{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9bbedb7-f817-4fb1-b1ed-a92c4fde40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.tasks.python.vision import drawing_utils as mp_drawing\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pyttsx3\n",
    "import joblib\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2eebd-d208-4cd4-97ff-4a96915b8475",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5e7b12-f0e8-4195-ad81-b2957703abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the landmarker model files\n",
    "POSE_MODEL_PATH = 'pose_landmarker_heavy.task'\n",
    "HAND_MODEL_PATH = 'hand_landmarker.task'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462be278-2c03-4bb7-aa67-b28ac9f22117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, pose_landmarker, hand_landmarker):\n",
    "    # Convert the image from BGR to RGB\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "    \n",
    "    # Process the image for pose and hands\n",
    "    pose_results = pose_landmarker.detect(mp_image)\n",
    "    hand_results = hand_landmarker.detect(mp_image)\n",
    "    \n",
    "    return pose_results, hand_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3169eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual definition of hand connections\n",
    "HAND_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4), (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "    (5, 9), (9, 10), (10, 11), (11, 12), (9, 13), (13, 14), (14, 15),\n",
    "    (15, 16), (13, 17), (0, 17), (17, 18), (18, 19), (19, 20)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f45abb-d088-4229-83ba-a5068ee12a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, hand_results, flash_active=False):\n",
    "    if not hand_results.hand_landmarks:\n",
    "        return\n",
    "    \n",
    "    # Determine color: Green if flashing, Blue/Black if not\n",
    "    color = (0, 255, 0) if flash_active else (255, 0, 0)\n",
    "    thickness = 3 if flash_active else 2\n",
    "    \n",
    "    for hand_landmarks in hand_results.hand_landmarks:\n",
    "        # Convert landmarks to pixel coordinates\n",
    "        h, w, _ = image.shape\n",
    "        pixel_landmarks = []\n",
    "        for landmark in hand_landmarks:\n",
    "            px, py = int(landmark.x * w), int(landmark.y * h)\n",
    "            pixel_landmarks.append((px, py))\n",
    "            cv2.circle(image, (px, py), 3, color, -1)\n",
    "            \n",
    "        # Draw connections\n",
    "        for connection in HAND_CONNECTIONS:\n",
    "            start_idx, end_idx = connection\n",
    "            cv2.line(image, pixel_landmarks[start_idx], pixel_landmarks[end_idx], color, thickness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b3697-ff3e-41e1-9b38-4dedcf6bed4f",
   "metadata": {},
   "source": [
    "## Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20da807-b757-494d-acf0-063246071697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', 150)\n",
    "\n",
    "    #Setting the voice\n",
    "    voices = engine.getProperty('voices')\n",
    "    engine.setProperty('voice', voices[1].id)\n",
    "\n",
    "    #Text input\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2ee3b-78eb-45e1-afbe-b3352d017ed1",
   "metadata": {},
   "source": [
    "# Make Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6bad1e-6b1c-46ec-83d4-a3cdaad01715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\signsema\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 1.1.0 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\miniconda3\\envs\\signsema\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator MLPClassifier from version 1.1.0 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_L = joblib.load('MP_model_head.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4306c9ab-b3ba-474a-ac19-4f008daeab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_output(sentence, sentence_out):\n",
    "    try:\n",
    "        with open('multi_sign.csv', 'r') as multisign_file:\n",
    "            sign_list_reader = csv.reader(multisign_file)\n",
    "            for row in sign_list_reader:\n",
    "                if len(sentence) >= 2:\n",
    "                    if sentence[-1] == row[-1] and sentence[-2] == row[-2]:\n",
    "                        sentence_out.append(row[0])\n",
    "                        break\n",
    "    except FileNotFoundError:\n",
    "        print(\"multi_sign.csv not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7faf98a2-fa73-4494-bbd5-f681d94103e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect(vidsource):\n",
    "    \n",
    "    sentence = []\n",
    "    sentence_out = []\n",
    "    predictions = []\n",
    "    last_sign_list = []\n",
    "    one_sign_list = []\n",
    "    \n",
    "    threshold = 0.9\n",
    "    pr = 3\n",
    "    pTime = 0\n",
    "    cTime = 0\n",
    "    \n",
    "    flash_counter = 0  # Counter for the 'flash green' effect\n",
    "    \n",
    "    # Loading complex signs mechanism\n",
    "    try:\n",
    "        with open('multi_sign.csv', 'r') as multisign_file:\n",
    "            sign_list_reader = csv.reader(multisign_file)\n",
    "            for row in sign_list_reader:\n",
    "                if row:\n",
    "                    last_sign_list.append(row[-1])\n",
    "    except FileNotFoundError:\n",
    "        print(\"multi_sign.csv not found\")\n",
    "    \n",
    "    # Loading simple signs\n",
    "    try:\n",
    "        with open('single_sign.csv', 'r') as singlesign_file:\n",
    "            singlesign_list_reader = csv.reader(singlesign_file)\n",
    "            for row in singlesign_list_reader:\n",
    "                if row:\n",
    "                    one_sign_list.append(row[0])\n",
    "    except FileNotFoundError:\n",
    "        print(\"single_sign.csv not found\")\n",
    "\n",
    "    # MediaPipe Task Setup\n",
    "    pose_options = vision.PoseLandmarkerOptions(\n",
    "        base_options=python.BaseOptions(model_asset_path=POSE_MODEL_PATH),\n",
    "        running_mode=vision.RunningMode.IMAGE\n",
    "    )\n",
    "    hand_options = vision.HandLandmarkerOptions(\n",
    "        base_options=python.BaseOptions(model_asset_path=HAND_MODEL_PATH),\n",
    "        running_mode=vision.RunningMode.IMAGE,\n",
    "        num_hands=2\n",
    "    )\n",
    "\n",
    "    cap = cv2.VideoCapture(vidsource)\n",
    "    \n",
    "    with vision.PoseLandmarker.create_from_options(pose_options) as pose_landmarker, \\\n",
    "         vision.HandLandmarker.create_from_options(hand_options) as hand_landmarker:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process frame\n",
    "            pose_results, hand_results = mediapipe_detection(frame, pose_landmarker, hand_landmarker)\n",
    "            image = frame.copy()\n",
    "            \n",
    "            # Draw landmarks with flash effect\n",
    "            draw_styled_landmarks(image, hand_results, flash_active=(flash_counter > 0))\n",
    "            if flash_counter > 0:\n",
    "                flash_counter -= 1\n",
    "\n",
    "            # Extract landmarks\n",
    "            # Hand landmarks extraction\n",
    "            lh_row = list(np.zeros(21*3))\n",
    "            rh_row = list(np.zeros(21*3))\n",
    "            \n",
    "            if hand_results.hand_landmarks:\n",
    "                for idx, handedness in enumerate(hand_results.handedness):\n",
    "                    label = handedness[0].category_name\n",
    "                    landmarks = hand_results.hand_landmarks[idx]\n",
    "                    flat_lms = list(np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten())\n",
    "                    \n",
    "                    if label == 'Left':\n",
    "                        lh_row = flat_lms\n",
    "                    elif label == 'Right':\n",
    "                        rh_row = flat_lms\n",
    "\n",
    "            # Pose (Head) extraction\n",
    "            head = list(np.zeros(1*3))\n",
    "            if pose_results.pose_landmarks:\n",
    "                # Use nose landmark (index 0) as 'head'\n",
    "                nose = pose_results.pose_landmarks[0][0]\n",
    "                if nose.visibility > 0.8:\n",
    "                    head = [nose.x, nose.y, nose.z]\n",
    "            \n",
    "            # Concatenate rows for model input\n",
    "            row = lh_row + rh_row + head\n",
    "\n",
    "            # Make Detections\n",
    "            # Ensure the row has exactly 129 features\n",
    "            if len(row) == 129:\n",
    "                X = pd.DataFrame([row])\n",
    "                sign_class = model_L.predict(X)[0]\n",
    "                sign_prob = model_L.predict_proba(X)[0]\n",
    "\n",
    "                # Sentence Logic\n",
    "                max_prob = sign_prob[np.argmax(sign_prob)]\n",
    "                if max_prob > threshold:\n",
    "                    predictions.append(sign_class)\n",
    "\n",
    "                    if len(predictions) >= pr and predictions[-pr:] == [sign_class]*pr:\n",
    "                        if len(sentence) > 0:\n",
    "                            if sign_class != sentence[-1]:\n",
    "                                sentence.append(sign_class)\n",
    "                                flash_counter = 5  # Flash green for 5 frames\n",
    "                                \n",
    "                                if sentence[-1] in last_sign_list:\n",
    "                                    sign_output(sentence, sentence_out)\n",
    "                                \n",
    "                                if sentence[-1] in one_sign_list:\n",
    "                                    sentence_out.append(sign_class)\n",
    "                        else:\n",
    "                            sentence.append(sign_class)\n",
    "                            if sentence[-1] in one_sign_list:\n",
    "                                    sentence_out.append(sign_class)\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]\n",
    "                    \n",
    "            if len(sentence_out) > 6:\n",
    "                    sentence_out = sentence_out[-6:]\n",
    "\n",
    "            # UI Overlay\n",
    "            cv2.rectangle(image, (0,0), (640, 40), (0,0,0), -1)\n",
    "            cv2.putText(image, ' '.join(sentence), (3, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.rectangle(image, (0, 45), (640, 85), (255,0,0), -1)\n",
    "            cv2.putText(image, ' '.join(sentence_out), (3, 75),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # FPS\n",
    "            cTime = time.time()\n",
    "            fps = 1 / (cTime - pTime) if (cTime - pTime) > 0 else 0\n",
    "            pTime = cTime\n",
    "\n",
    "            cv2.putText(image, f\"FPS: {int(fps)}\", (10, 450), cv2.FONT_HERSHEY_PLAIN, 2, (0,255,0), 2)\n",
    "\n",
    "            # Show to screen\n",
    "            cv2.imshow('SignSema Detection', image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31be636-57fb-439f-970a-2e3fef35f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signsema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
